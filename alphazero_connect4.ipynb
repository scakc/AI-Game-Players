{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alphazero_connect4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FDpXX5MNVbF8",
        "OM6uCOyxVfZe",
        "1qfhCDUbVqVH",
        "02-iL09RWFAU",
        "c5PD5_Weotxf",
        "Q5cKWnLpo1Wm",
        "Rx41cPKFpguh",
        "D-GKeUSXpuxi"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlT0WuoBvUtw",
        "colab_type": "text"
      },
      "source": [
        "# Connect4 Game Play Alpha-Zero Model (Deep Mind)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZZpcRa2fcBA",
        "colab_type": "text"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz0UXDi1PKKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "import os\n",
        "import collections\n",
        "import math\n",
        "import copy\n",
        "import datetime\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output as clr\n",
        "matplotlib.use(\"Agg\")\n",
        "logging.basicConfig(format='%(asctime)s [%(levelname)s]: %(message)s', \\\n",
        "                    datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
        "logger = logging.getLogger('log.txt')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDpXX5MNVbF8",
        "colab_type": "text"
      },
      "source": [
        "### Defining the Game Board and Other Enviornment functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTSpxcsGq7od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class board:\n",
        "    def __init__(self):\n",
        "        self.init_board = np.zeros([6,7]).astype(str)\n",
        "        self.init_board[self.init_board == \"0.0\"] = \" \"\n",
        "        self.player = 0\n",
        "        self.current_board = self.init_board\n",
        "        \n",
        "    def drop_piece(self, column):\n",
        "        if self.current_board[0, column] != \" \":\n",
        "            return \"Invalid move\"\n",
        "        else:\n",
        "            row = 0; pos = \" \"\n",
        "            while (pos == \" \"):\n",
        "                if row == 6:\n",
        "                    row += 1\n",
        "                    break\n",
        "                pos = self.current_board[row, column]\n",
        "                row += 1\n",
        "            if self.player == 0:\n",
        "                self.current_board[row-2, column] = \"O\"\n",
        "                self.player = 1\n",
        "            elif self.player == 1:\n",
        "                self.current_board[row-2, column] = \"X\"\n",
        "                self.player = 0\n",
        "    \n",
        "    def check_winner(self):\n",
        "        if self.player == 1:\n",
        "            for row in range(6):\n",
        "                for col in range(7):\n",
        "                    if self.current_board[row, col] != \" \":\n",
        "                        # rows\n",
        "                        try:\n",
        "                            if self.current_board[row, col] == \"O\" and self.current_board[row + 1, col] == \"O\" and \\\n",
        "                                self.current_board[row + 2, col] == \"O\" and self.current_board[row + 3, col] == \"O\":\n",
        "                                #print(\"row\")\n",
        "                                return True\n",
        "                        except IndexError:\n",
        "                            next\n",
        "                        # columns\n",
        "                        try:\n",
        "                            if self.current_board[row, col] == \"O\" and self.current_board[row, col + 1] == \"O\" and \\\n",
        "                                self.current_board[row, col + 2] == \"O\" and self.current_board[row, col + 3] == \"O\":\n",
        "                                #print(\"col\")\n",
        "                                return True\n",
        "                        except IndexError:\n",
        "                            next\n",
        "                        # \\ diagonal\n",
        "                        try:\n",
        "                            if self.current_board[row, col] == \"O\" and self.current_board[row + 1, col + 1] == \"O\" and \\\n",
        "                                self.current_board[row + 2, col + 2] == \"O\" and self.current_board[row + 3, col + 3] == \"O\":\n",
        "                                #print(\"\\\\\")\n",
        "                                return True\n",
        "                        except IndexError:\n",
        "                            next\n",
        "                        # / diagonal\n",
        "                        try:\n",
        "                            if self.current_board[row, col] == \"O\" and self.current_board[row + 1, col - 1] == \"O\" and \\\n",
        "                                self.current_board[row + 2, col - 2] == \"O\" and self.current_board[row + 3, col - 3] == \"O\"\\\n",
        "                                and (col-3) >= 0:\n",
        "                                #print(\"/\")\n",
        "                                return True\n",
        "                        except IndexError:\n",
        "                            next\n",
        "        if self.player == 0:\n",
        "            for row in range(6):\n",
        "                for col in range(7):\n",
        "                    if self.current_board[row, col] != \" \":\n",
        "                        # rows\n",
        "                        try:\n",
        "                            if self.current_board[row, col] == \"X\" and self.current_board[row + 1, col] == \"X\" and \\\n",
        "                                self.current_board[row + 2, col] == \"X\" and self.current_board[row + 3, col] == \"X\":\n",
        "                                return True\n",
        "                        except IndexError:\n",
        "                            next\n",
        "                        # columns\n",
        "                        try:\n",
        "                            if self.current_board[row, col] == \"X\" and self.current_board[row, col + 1] == \"X\" and \\\n",
        "                                self.current_board[row, col + 2] == \"X\" and self.current_board[row, col + 3] == \"X\":\n",
        "                                return True\n",
        "                        except IndexError:\n",
        "                            next\n",
        "                        # \\ diagonal\n",
        "                        try:\n",
        "                            if self.current_board[row, col] == \"X\" and self.current_board[row + 1, col + 1] == \"X\" and \\\n",
        "                                self.current_board[row + 2, col + 2] == \"X\" and self.current_board[row + 3, col + 3] == \"X\":\n",
        "                                return True\n",
        "                        except IndexError:\n",
        "                            next\n",
        "                        # / diagonal\n",
        "                        try:\n",
        "                            if self.current_board[row, col] == \"X\" and self.current_board[row + 1, col - 1] == \"X\" and \\\n",
        "                                self.current_board[row + 2, col - 2] == \"X\" and self.current_board[row + 3, col - 3] == \"X\"\\\n",
        "                                and (col-3) >= 0:\n",
        "                                return True\n",
        "                        except IndexError:\n",
        "                            next\n",
        "    def actions(self): # returns all possible moves\n",
        "        acts = []\n",
        "        for col in range(7):\n",
        "            if self.current_board[0, col] == \" \":\n",
        "                acts.append(col)\n",
        "        return acts"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfZJsfAHTu81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_board(board):\n",
        "    board_state = board.current_board\n",
        "    encoded = np.zeros([6,7,3]).astype(int) # 1st axis for O, 2nd axis for X and last axis for player position.\n",
        "    encoder_dict = {\"O\":0, \"X\":1}\n",
        "    for row in range(6):\n",
        "        for col in range(7):\n",
        "            if board_state[row,col] != \" \":\n",
        "                encoded[row, col, encoder_dict[board_state[row,col]]] = 1 # Defining the state of game.\n",
        "    if board.player == 1: ## If player has to move next.\n",
        "        encoded[:,:,2] = 1 # player to move\n",
        "    return encoded\n",
        "\n",
        "def decode_board(encoded):\n",
        "    decoded = np.zeros([6,7]).astype(str)\n",
        "    decoded[decoded == \"0.0\"] = \" \"\n",
        "    decoder_dict = {0:\"O\", 1:\"X\"}\n",
        "    for row in range(6):\n",
        "        for col in range(7):\n",
        "            for k in range(2):\n",
        "                if encoded[row, col, k] == 1:\n",
        "                    decoded[row, col] = decoder_dict[k]\n",
        "    cboard = board()\n",
        "    cboard.current_board = decoded\n",
        "    cboard.player = encoded[0,0,2]\n",
        "    return cboard\n",
        "    \n",
        "def view_board(np_data, fmt='{:s}', bkg_colors=['pink', 'pink']):\n",
        "    data = pd.DataFrame(np_data, columns=['0','1','2','3','4','5','6'])\n",
        "    fig, ax = plt.subplots(figsize=[7,7])\n",
        "    ax.set_axis_off()\n",
        "    tb = Table(ax, bbox=[0,0,1,1])\n",
        "    nrows, ncols = data.shape\n",
        "    width, height = 1.0 / ncols, 1.0 / nrows\n",
        "\n",
        "    for (i,j), val in np.ndenumerate(data):\n",
        "        idx = [j % 2, (j + 1) % 2][i % 2]\n",
        "        color = bkg_colors[idx]\n",
        "\n",
        "        tb.add_cell(i, j, width, height, text=fmt.format(val), \n",
        "                    loc='center', facecolor=color)\n",
        "\n",
        "    for i, label in enumerate(data.index):\n",
        "        tb.add_cell(i, -1, width, height, text=label, loc='right', \n",
        "                    edgecolor='none', facecolor='none')\n",
        "\n",
        "    for j, label in enumerate(data.columns):\n",
        "        tb.add_cell(-1, j, width, height/2, text=label, loc='center', \n",
        "                           edgecolor='none', facecolor='none')\n",
        "    tb.set_fontsize(24)\n",
        "    ax.add_table(tb)\n",
        "    return fig"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgtQ-DYCZVhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class board_data(Dataset):\n",
        "    def __init__(self, dataset): # dataset = np.array of (s, p, v)\n",
        "        self.X = dataset[:,0]\n",
        "        self.y_p, self.y_v = dataset[:,1], dataset[:,2]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        return np.int64(self.X[idx].transpose(2,0,1)), self.y_p[idx], self.y_v[idx]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM6uCOyxVfZe",
        "colab_type": "text"
      },
      "source": [
        "### Defining the Agent Network for policy and value heads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKMfQv34PMd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InBlock, self).__init__()\n",
        "        self.action_size = 7\n",
        "        self.conv1 = nn.Conv2d(3, 128, 3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,3,6,7) # batch_size x channels x board_x x board_y\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5wIIpbtRFqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, inplanes=128, planes=128, stride=1, downsample=None):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        z = F.relu(self.bn1(self.conv1(x)))\n",
        "        z = self.bn2(self.conv2(z))\n",
        "        z += res\n",
        "        z = F.relu(z)\n",
        "        return z"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bm-_yMxR2zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OutBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OutBlock, self).__init__()\n",
        "        ## Value Head ##\n",
        "        self.conv1 = nn.Conv2d(128, 3, kernel_size=1)\n",
        "        self.bn1 = nn.BatchNorm2d(3)\n",
        "        self.fc1 = nn.Linear(3*6*7, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "        ## Policy Head ##        \n",
        "        self.conv2 = nn.Conv2d(128, 32, kernel_size=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        self.fc3 = nn.Linear(6*7*32, 7)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        ## Value Head ##\n",
        "        v = F.relu(self.bn1(self.conv1(x)))\n",
        "        v = v.view(-1, 3*6*7)  # batch_size X channel X height X width\n",
        "        v = F.relu(self.fc1(v))\n",
        "        v = torch.tanh(self.fc2(v))\n",
        "        \n",
        "        ## Policy Head ##\n",
        "        p = F.relu(self.bn2(self.conv2(x))) \n",
        "        p = p.view(-1, 6*7*32)\n",
        "        p = self.fc3(p)\n",
        "        p = self.logsoftmax(p).exp()\n",
        "        return p, v"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNqf1aZNS-L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RLAgent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RLAgent, self).__init__()\n",
        "        self.inblock = InBlock()\n",
        "        for block in range(19):\n",
        "            setattr(self, \"res_%i\" % block,ResBlock())\n",
        "        self.outblock = OutBlock()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.inblock(x)\n",
        "        for block in range(19):\n",
        "            x = getattr(self, \"res_%i\" % block)(x)\n",
        "        x = self.outblock(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qfhCDUbVqVH",
        "colab_type": "text"
      },
      "source": [
        "### Defining Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcAHvJ60UW7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlphaLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlphaLoss, self).__init__()\n",
        "        self.eps = 10e-8\n",
        "\n",
        "    def forward(self, y_value, value, y_policy, policy):\n",
        "        value_error = (value - y_value) ** 2 ## MSE loss for desired value vs predicted value by the network.\n",
        "        policy_error = torch.sum((-policy* (self.eps + y_policy.float()).float().log()), 1) # Cross entropy loss between the predicted and desired policy\n",
        "        total_error = (value_error.view(-1).float() + policy_error).mean()\n",
        "        return total_error"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02-iL09RWFAU",
        "colab_type": "text"
      },
      "source": [
        "### Monte Carlo Tree Search\n",
        "\n",
        "1. Select\n",
        "2. Expand\n",
        "3. Backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWf8tCHWVEid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_as_pickle(filename, data):\n",
        "    completeName = os.path.join(\"./datasets/\",\\\n",
        "                                filename)\n",
        "    with open(completeName, 'wb') as output:\n",
        "        pickle.dump(data, output)\n",
        "\n",
        "def load_pickle(filename):\n",
        "    completeName = os.path.join(\"./datasets/\",\\\n",
        "                                filename)\n",
        "    with open(completeName, 'rb') as pkl_file:\n",
        "        data = pickle.load(pkl_file)\n",
        "    return data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbBwUvveaIZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UCTNode():\n",
        "    def __init__(self, game, move, parent=None):\n",
        "        self.game = game # state s\n",
        "        self.move = move # action index\n",
        "        self.is_expanded = False # is leaf or not\n",
        "        self.parent = parent  # Link to parent\n",
        "        self.children = {} # Link/s to children\n",
        "        ###  Assuming only 7 next states can be reached ###\n",
        "        self.child_priors = np.zeros([7], dtype=np.float32)\n",
        "        self.child_total_value = np.zeros([7], dtype=np.float32)\n",
        "        self.child_number_visits = np.zeros([7], dtype=np.float32)\n",
        "        self.action_idxes = []\n",
        "        \n",
        "    @property\n",
        "    def number_visits(self):\n",
        "        # Get the count of times this state action node has been visited during the overall play.\n",
        "        return self.parent.child_number_visits[self.move]\n",
        "\n",
        "    @number_visits.setter\n",
        "    def number_visits(self, value):\n",
        "        # Set the count of times this state action node has been visited during the overall play.\n",
        "        self.parent.child_number_visits[self.move] = value\n",
        "    \n",
        "    @property\n",
        "    def total_value(self):\n",
        "        # Gets the total number of times this nodes parent has taken this nodes action.\n",
        "        return self.parent.child_total_value[self.move]\n",
        "    \n",
        "    @total_value.setter\n",
        "    def total_value(self, value):\n",
        "        # Sets the total number of times this nodes parent has taken this nodes action.\n",
        "        self.parent.child_total_value[self.move] = value\n",
        "    \n",
        "    def child_Q(self):\n",
        "        # Average value of this node (Exploitation)\n",
        "        return self.child_total_value / (1 + self.child_number_visits)\n",
        "    \n",
        "    def child_U(self):\n",
        "        # Exploration factor for this node\n",
        "        return math.sqrt(self.number_visits) * (\n",
        "            abs(self.child_priors) / (1 + self.child_number_visits))\n",
        "    \n",
        "    def best_child(self):\n",
        "        # Returns the best possible move for Exploitaion Exploration model.\n",
        "        # If a node has been selected alot of times the U value decreases.\n",
        "        if self.action_idxes != []:\n",
        "            bestmove = self.child_Q() + self.child_U()\n",
        "            bestmove = self.action_idxes[np.argmax(bestmove[self.action_idxes])]\n",
        "        else:\n",
        "            bestmove = np.argmax(self.child_Q() + self.child_U())\n",
        "        return bestmove\n",
        "    \n",
        "    def select_leaf(self):\n",
        "        # Selects the Lead node (1st step of MCTS)\n",
        "        # Selects the best node (based on Exploration and Exploitation) recursively.\n",
        "        current = self\n",
        "        while current.is_expanded:\n",
        "          best_move = current.best_child()\n",
        "          current = current.maybe_add_child(best_move)\n",
        "        return current\n",
        "    \n",
        "    def add_dirichlet_noise(self,action_idxs,child_priors):\n",
        "        valid_child_priors = child_priors[action_idxs] # select only legal moves entries in child_priors array\n",
        "        valid_child_priors = 0.75*valid_child_priors + 0.25*np.random.dirichlet(np.zeros([len(valid_child_priors)], \\\n",
        "                                                                                          dtype=np.float32)+192)\n",
        "        child_priors[action_idxs] = valid_child_priors\n",
        "        return child_priors\n",
        "    \n",
        "    def expand(self, child_priors):\n",
        "        # Expands the current node.\n",
        "        self.is_expanded = True # Bool Flag variable\n",
        "        action_idxs = self.game.actions() # Get possible indexes of action that the current player can take.\n",
        "        c_p = child_priors # The prior values of this nodes child\n",
        "        if action_idxs == []: # If no moves are possible dont extend\n",
        "            self.is_expanded = False\n",
        "        self.action_idxes = action_idxs\n",
        "        c_p[[i for i in range(len(child_priors)) if i not in action_idxs]] = 0.000000000 # mask all illegal actions\n",
        "        if self.parent.parent == None: # add dirichlet noise to child_priors in root node childs to avoid prefereing one over other strongly.\n",
        "            c_p = self.add_dirichlet_noise(action_idxs,c_p)\n",
        "        self.child_priors = c_p\n",
        "    \n",
        "    def decode_n_move_pieces(self,board,move):\n",
        "        board.drop_piece(move) # Make a move\n",
        "        return board\n",
        "            \n",
        "    def maybe_add_child(self, move):\n",
        "        # Returns the children if its present else it creates it.\n",
        "        if move not in self.children:\n",
        "            copy_board = copy.deepcopy(self.game) # make copy of board\n",
        "            copy_board = self.decode_n_move_pieces(copy_board,move)\n",
        "            self.children[move] = UCTNode(\n",
        "              copy_board, move, parent=self)\n",
        "        return self.children[move]\n",
        "    \n",
        "    def backup(self, value_estimate: float):\n",
        "        # Backups the current node with its estimated value.\n",
        "        current = self\n",
        "        while current.parent is not None:\n",
        "            current.number_visits += 1\n",
        "            if current.game.player == 1: # same as current.parent.game.player = 0\n",
        "                current.total_value += (1*value_estimate) # value estimate +1 = O wins\n",
        "            elif current.game.player == 0: # same as current.parent.game.player = 1\n",
        "                current.total_value += (-1*value_estimate)\n",
        "            current = current.parent"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5PD5_Weotxf",
        "colab_type": "text"
      },
      "source": [
        "### MCTS with Self Play Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0OOuhFOgIaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DummyNode(object):\n",
        "    # A dummy node\n",
        "    def __init__(self):\n",
        "        self.parent = None\n",
        "        self.child_total_value = collections.defaultdict(float)\n",
        "        self.child_number_visits = collections.defaultdict(float)\n",
        "\n",
        "## Function for randomly searching the tree from a given game state.\n",
        "def UCT_search(game_state, num_reads,net,temp):\n",
        "    root = UCTNode(game_state, move=None, parent=DummyNode())\n",
        "    for i in range(num_reads):\n",
        "        leaf = root.select_leaf() # Selects a leaf\n",
        "        encoded_s = encode_board(leaf.game)  # Encodes the current state of game at selected leaf\n",
        "        encoded_s = encoded_s.transpose(2,0,1) \n",
        "        encoded_s = torch.from_numpy(encoded_s).float().cuda()\n",
        "        child_priors, value_estimate = net(encoded_s) # Estimates the Value and Policy vectors using the Neural Network.\n",
        "        child_priors = child_priors.detach().cpu().numpy().reshape(-1)\n",
        "        value_estimate = value_estimate.item()\n",
        "        if leaf.game.check_winner() == True or leaf.game.actions() == []: # if somebody won or draw\n",
        "            leaf.backup(value_estimate) # Backup this leaf node and start again.\n",
        "            continue\n",
        "        leaf.expand(child_priors) # need to make sure valid moves (if expandable the node will be expanded)\n",
        "        leaf.backup(value_estimate) # Backup this leaf node and start again.\n",
        "    return root\n",
        "\n",
        "def do_decode_n_move_pieces(board,move):\n",
        "    board.drop_piece(move) # Make a move in board\n",
        "    return board\n",
        "\n",
        "def get_policy(root, temp=1):\n",
        "    ## Gives the policy for the input node\n",
        "    #policy = np.zeros([7], dtype=np.float32)\n",
        "    #for idx in np.where(root.child_number_visits!=0)[0]:\n",
        "    #    policy[idx] = ((root.child_number_visits[idx])**(1/temp))/sum(root.child_number_visits**(1/temp))\n",
        "    return ((root.child_number_visits)**(1/temp))/sum(root.child_number_visits**(1/temp))\n",
        "\n",
        "# Performs a self play\n",
        "def MCTS_self_play(connectnet, num_games, start_idx, cpu, args, iteration):\n",
        "    logger.info(\"[CPU: %d]: Starting MCTS self-play...\" % cpu)\n",
        "    # Creating a folder for storing the generated gameplays.\n",
        "    if not os.path.isdir(\"./datasets/iter_%d\" % iteration):\n",
        "        if not os.path.isdir(\"datasets\"):\n",
        "            os.mkdir(\"datasets\")\n",
        "        os.mkdir(\"datasets/iter_%d\" % iteration)\n",
        "        \n",
        "    for idxx in tqdm(range(start_idx, num_games + start_idx)):\n",
        "        current_board = board()\n",
        "        checkmate = False\n",
        "        dataset = [] # to get state, policy, value for neural network training\n",
        "        states = []\n",
        "        value = 0\n",
        "        move_count = 0\n",
        "        while checkmate == False and current_board.actions() != []:\n",
        "            # Selecting the temperature\n",
        "            if move_count < 11:\n",
        "                t = args.temperature_MCTS\n",
        "            else:\n",
        "                t = 0.1\n",
        "            # Save the current board state of the game for training the agent\n",
        "            states.append(copy.deepcopy(current_board.current_board))\n",
        "            board_state = copy.deepcopy(encode_board(current_board))\n",
        "            # Perform a UCT search from the current board state and get the estimated optimal policy\n",
        "            root = UCT_search(current_board,777,connectnet,t)\n",
        "            clr(wait=True)\n",
        "            policy = get_policy(root, t); print(\"[CPU: %d]: Game %d POLICY:\\n \" % (cpu, idxx), policy)\n",
        "            # Make a move based on obtained policy\n",
        "            current_board = do_decode_n_move_pieces(current_board,\\\n",
        "                                                    np.random.choice(np.array([0,1,2,3,4,5,6]), \\\n",
        "                                                                     p = policy)) # decode move and move piece(s)\n",
        "            # Add the current board state and policy obtained for training\n",
        "            dataset.append([board_state,policy])\n",
        "            print(\"[Iteration: %d CPU: %d]: Game %d CURRENT BOARD:\\n\" % (iteration, cpu, idxx), current_board.current_board,current_board.player); print(\" \")\n",
        "            # Final states values set as 1 or -1 depending on who wins.\n",
        "            if current_board.check_winner() == True: # if somebody won\n",
        "                if current_board.player == 0: # black wins\n",
        "                    value = -1\n",
        "                elif current_board.player == 1: # white wins\n",
        "                    value = 1\n",
        "                checkmate = True\n",
        "            move_count += 1 # Stores the number of moves taken\n",
        "        # Creating a state, policy, value sets for training.\n",
        "        dataset_p = []\n",
        "        for idx,data in enumerate(dataset):\n",
        "            s,p = data\n",
        "            if idx == 0:\n",
        "                dataset_p.append([s,p,0]) # Zero value for root node\n",
        "            else:\n",
        "                dataset_p.append([s,p,value]) # Value for all other states is set a   '''May be wrong here check later''' \n",
        "        del dataset\n",
        "        # Saving the created data\n",
        "        save_as_pickle(\"iter_%d/\" % iteration +\\\n",
        "                       \"dataset_iter%d_cpu%i_%i_%s\" % (iteration, cpu, idxx, datetime.datetime.today().strftime(\"%Y-%m-%d\")), dataset_p)\n",
        "\n",
        "  \n",
        "def run_MCTS(args, start_idx=0, iteration=0):\n",
        "    net_to_play=\"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration)\n",
        "    net = RLAgent()\n",
        "    cuda = torch.cuda.is_available()\n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    \n",
        "    if args.MCTS_num_processes > 1:\n",
        "        logger.info(\"Preparing model for multi-process MCTS...\")\n",
        "        mp.set_start_method(\"spawn\",force=True)\n",
        "        net.share_memory()\n",
        "        net.eval()\n",
        "    \n",
        "        current_net_filename = os.path.join(\"./model_data/\",\\\n",
        "                                        net_to_play)\n",
        "        if os.path.isfile(current_net_filename):\n",
        "            checkpoint = torch.load(current_net_filename)\n",
        "            net.load_state_dict(checkpoint['state_dict'])\n",
        "            logger.info(\"Loaded %s model.\" % current_net_filename)\n",
        "        else:\n",
        "            torch.save({'state_dict': net.state_dict()}, os.path.join(\"./model_data/\",\\\n",
        "                        net_to_play))\n",
        "            logger.info(\"Initialized model.\")\n",
        "        \n",
        "        processes = []\n",
        "        if args.MCTS_num_processes > mp.cpu_count():\n",
        "            num_processes = mp.cpu_count()\n",
        "            logger.info(\"Required number of processes exceed number of CPUs! Setting MCTS_num_processes to %d\" % num_processes)\n",
        "        else:\n",
        "            num_processes = args.MCTS_num_processes\n",
        "        \n",
        "        logger.info(\"Spawning %d processes...\" % num_processes)\n",
        "        with torch.no_grad():\n",
        "            for i in range(num_processes):\n",
        "                # Runs Self Play on multiple processes\n",
        "                p = mp.Process(target=MCTS_self_play, args=(net, args.num_games_per_MCTS_process, start_idx, i, args, iteration))\n",
        "                p.start()\n",
        "                processes.append(p)\n",
        "            for p in processes:\n",
        "                p.join()\n",
        "        logger.info(\"Finished multi-process MCTS!\")\n",
        "    \n",
        "    elif args.MCTS_num_processes == 1:\n",
        "        logger.info(\"Preparing model for MCTS...\")\n",
        "        net.eval()\n",
        "        \n",
        "        current_net_filename = os.path.join(\"./model_data/\",\\\n",
        "                                        net_to_play)\n",
        "        if os.path.isfile(current_net_filename):\n",
        "            checkpoint = torch.load(current_net_filename)\n",
        "            net.load_state_dict(checkpoint['state_dict'])\n",
        "            logger.info(\"Loaded %s model.\" % current_net_filename)\n",
        "        else:\n",
        "            torch.save({'state_dict': net.state_dict()}, os.path.join(\"./model_data/\",\\\n",
        "                        net_to_play))\n",
        "            logger.info(\"Initialized model.\")\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            MCTS_self_play(net, args.num_games_per_MCTS_process, start_idx, 0, args, iteration)\n",
        "        logger.info(\"Finished MCTS!\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5cKWnLpo1Wm",
        "colab_type": "text"
      },
      "source": [
        "### Training Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNO5w8ypobZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_state(net, optimizer, scheduler, args, iteration, new_optim_state=True):\n",
        "    \"\"\" Loads saved model and optimizer states if exists \"\"\"\n",
        "    base_path = \"./model_data/\"\n",
        "    checkpoint_path = os.path.join(base_path, \"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration))\n",
        "    start_epoch, checkpoint = 0, None\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "    if checkpoint != None:\n",
        "        if (len(checkpoint) == 1) or (new_optim_state == True):\n",
        "            net.load_state_dict(checkpoint['state_dict'])\n",
        "            logger.info(\"Loaded checkpoint model %s.\" % checkpoint_path)\n",
        "        else:\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            net.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "            logger.info(\"Loaded checkpoint model %s, and optimizer, scheduler.\" % checkpoint_path)    \n",
        "    return start_epoch\n",
        "\n",
        "def load_results(iteration):\n",
        "    \"\"\" Loads saved results if exists \"\"\"\n",
        "    losses_path = \"./model_data/losses_per_epoch_iter%d.pkl\" % iteration\n",
        "    if os.path.isfile(losses_path):\n",
        "        losses_per_epoch = load_pickle(\"losses_per_epoch_iter%d.pkl\" % iteration)\n",
        "        logger.info(\"Loaded results buffer\")\n",
        "    else:\n",
        "        losses_per_epoch = []\n",
        "    return losses_per_epoch"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yspIsT9fN3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oc-UnECobN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, dataset, optimizer, scheduler, start_epoch, cpu, args, iteration):\n",
        "    torch.manual_seed(cpu)\n",
        "    cuda = torch.cuda.is_available()\n",
        "    net.train()\n",
        "    criterion = AlphaLoss()\n",
        "    \n",
        "    train_set = board_data(dataset)\n",
        "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
        "    losses_per_epoch = load_results(iteration + 1)\n",
        "    \n",
        "    logger.info(\"Starting training process...\")\n",
        "    update_size = len(train_loader)//10\n",
        "    print(\"Update step size: %d\" % update_size)\n",
        "    for epoch in range(start_epoch, args.num_epochs):\n",
        "        total_loss = 0.0\n",
        "        losses_per_batch = []\n",
        "        for i,data in enumerate(train_loader,0):\n",
        "            state, policy, value = data\n",
        "            state, policy, value = state.float(), policy.float(), value.float()\n",
        "            if cuda:\n",
        "                state, policy, value = state.cuda(), policy.cuda(), value.cuda()\n",
        "            policy_pred, value_pred = net(state) # policy_pred = torch.Size([batch, 4672]) value_pred = torch.Size([batch, 1])\n",
        "            loss = criterion(value_pred[:,0], value, policy_pred, policy)\n",
        "            loss = loss/args.gradient_acc_steps\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(net.parameters(), args.max_norm)\n",
        "            if (epoch % args.gradient_acc_steps) == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "            total_loss += loss.item()\n",
        "            if i % update_size == (update_size - 1):    # print every update_size-d mini-batches of size = batch_size\n",
        "                losses_per_batch.append(args.gradient_acc_steps*total_loss/update_size)\n",
        "                print('[Iteration %d] Process ID: %d [Epoch: %d, %5d/ %d points] total loss per batch: %.3f' %\n",
        "                      (iteration, os.getpid(), epoch + 1, (i + 1)*args.batch_size, len(train_set), losses_per_batch[-1]))\n",
        "                print(\"Policy (actual, predicted):\",policy[0].argmax().item(),policy_pred[0].argmax().item())\n",
        "                print(\"Policy data:\", policy[0]); print(\"Policy pred:\", policy_pred[0])\n",
        "                print(\"Value (actual, predicted):\", value[0].item(), value_pred[0,0].item())\n",
        "                #print(\"Conv grad: %.7f\" % net.conv.conv1.weight.grad.mean().item())\n",
        "                #print(\"Res18 grad %.7f:\" % net.res_18.conv1.weight.grad.mean().item())\n",
        "                print(\" \")\n",
        "                total_loss = 0.0\n",
        "        \n",
        "        scheduler.step()\n",
        "        if len(losses_per_batch) >= 1:\n",
        "            losses_per_epoch.append(sum(losses_per_batch)/len(losses_per_batch))\n",
        "        if (epoch % 2) == 0:\n",
        "            save_as_pickle(\"losses_per_epoch_iter%d.pkl\" % (iteration + 1), losses_per_epoch)\n",
        "            torch.save({\n",
        "                    'epoch': epoch + 1,\\\n",
        "                    'state_dict': net.state_dict(),\\\n",
        "                    'optimizer' : optimizer.state_dict(),\\\n",
        "                    'scheduler' : scheduler.state_dict(),\\\n",
        "                }, os.path.join(\"./model_data/\",\\\n",
        "                    \"%s_iter%d.pth.tar\" % (args.neural_net_name, (iteration + 1))))\n",
        "        '''\n",
        "        # Early stopping\n",
        "        if len(losses_per_epoch) > 50:\n",
        "            if abs(sum(losses_per_epoch[-4:-1])/3-sum(losses_per_epoch[-16:-13])/3) <= 0.00017:\n",
        "                break\n",
        "        '''\n",
        "    logger.info(\"Finished Training!\")\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(222)\n",
        "    ax.scatter([e for e in range(start_epoch, (len(losses_per_epoch) + start_epoch))], losses_per_epoch)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss per batch\")\n",
        "    ax.set_title(\"Loss vs Epoch\")\n",
        "    plt.savefig(os.path.join(\"./model_data/\", \"Loss_vs_Epoch_iter%d_%s.png\" % ((iteration + 1), datetime.datetime.today().strftime(\"%Y-%m-%d\"))))\n",
        "    plt.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTFbTgxRoa7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_connectnet(args, iteration, new_optim_state):\n",
        "    # gather data\n",
        "    logger.info(\"Loading training data...\")\n",
        "    data_path=\"./datasets/iter_%d/\" % iteration\n",
        "    datasets = []\n",
        "    for idx,file in enumerate(os.listdir(data_path)):\n",
        "        filename = os.path.join(data_path,file)\n",
        "        with open(filename, 'rb') as fo:\n",
        "            datasets.extend(pickle.load(fo, encoding='bytes'))\n",
        "    datasets = np.array(datasets)\n",
        "    logger.info(\"Loaded data from %s.\" % data_path)\n",
        "    \n",
        "    # train net\n",
        "    net = RLAgent()\n",
        "    cuda = torch.cuda.is_available()\n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=args.lr, betas=(0.8, 0.999))\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,100,150,200,250,300,400], gamma=0.77)\n",
        "    start_epoch = load_state(net, optimizer, scheduler, args, iteration, new_optim_state)\n",
        "    \n",
        "    train(net, datasets, optimizer, scheduler, start_epoch, 0, args, iteration)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx41cPKFpguh",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev05sd6QpEL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class arena():\n",
        "    def __init__(self, current_cnet, best_cnet):\n",
        "        self.current = current_cnet\n",
        "        self.best = best_cnet\n",
        "    \n",
        "    def play_round(self):\n",
        "        logger.info(\"Starting game round...\")\n",
        "        if np.random.uniform(0,1) <= 0.5:\n",
        "            white = self.current; black = self.best; w = \"current\"; b = \"best\"\n",
        "        else:\n",
        "            white = self.best; black = self.current; w = \"best\"; b = \"current\"\n",
        "        current_board = board()\n",
        "        checkmate = False\n",
        "        dataset = []\n",
        "        value = 0; t = 0.1\n",
        "        while checkmate == False and current_board.actions() != []:\n",
        "            dataset.append(copy.deepcopy(encode_board(current_board)))\n",
        "            print(\"\"); print(current_board.current_board)\n",
        "            if current_board.player == 0:\n",
        "                root = UCT_search(current_board,777,white,t)\n",
        "                policy = get_policy(root, t); print(\"Policy: \", policy, \"white = %s\" %(str(w)))\n",
        "            elif current_board.player == 1:\n",
        "                root = UCT_search(current_board,777,black,t)\n",
        "                policy = get_policy(root, t); print(\"Policy: \", policy, \"black = %s\" %(str(b)))\n",
        "            current_board = do_decode_n_move_pieces(current_board,\\\n",
        "                                                    np.random.choice(np.array([0,1,2,3,4,5,6]), \\\n",
        "                                                                     p = policy)) # decode move and move piece(s)\n",
        "            if current_board.check_winner() == True: # someone wins\n",
        "                if current_board.player == 0: # black wins\n",
        "                    value = -1\n",
        "                elif current_board.player == 1: # white wins\n",
        "                    value = 1\n",
        "                checkmate = True\n",
        "        dataset.append(encode_board(current_board))\n",
        "        if value == -1:\n",
        "            dataset.append(f\"{b} as black wins\")\n",
        "            return b, dataset\n",
        "        elif value == 1:\n",
        "            dataset.append(f\"{w} as white wins\")\n",
        "            return w, dataset\n",
        "        else:\n",
        "            dataset.append(\"Nobody wins\")\n",
        "            return None, dataset\n",
        "    \n",
        "    def evaluate(self, num_games, cpu):\n",
        "        current_wins = 0\n",
        "        logger.info(\"[CPU %d]: Starting games...\" % cpu)\n",
        "        for i in range(num_games):\n",
        "            with torch.no_grad():\n",
        "                winner, dataset = self.play_round(); print(\"%s wins!\" % winner)\n",
        "            if winner == \"current\":\n",
        "                current_wins += 1\n",
        "            save_as_pickle(\"evaluate_net_dataset_cpu%i_%i_%s_%s\" % (cpu,i,datetime.datetime.today().strftime(\"%Y-%m-%d\"),\\\n",
        "                                                                     str(winner)),dataset)\n",
        "        print(\"Current_net wins ratio: %.5f\" % (current_wins/num_games))\n",
        "        save_as_pickle(\"wins_cpu_%i\" % (cpu),\\\n",
        "                                             {\"best_win_ratio\": current_wins/num_games, \"num_games\":num_games})\n",
        "        logger.info(\"[CPU %d]: Finished arena games!\" % cpu)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G1w7XxzpDxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fork_process(arena_obj, num_games, cpu): # make arena picklable\n",
        "    arena_obj.evaluate(num_games, cpu)\n",
        "\n",
        "def evaluate_nets(args, iteration_1, iteration_2) :\n",
        "    logger.info(\"Loading nets...\")\n",
        "    current_net=\"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration_2); best_net=\"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration_1)\n",
        "    current_net_filename = os.path.join(\"./model_data/\",\\\n",
        "                                    current_net)\n",
        "    best_net_filename = os.path.join(\"./model_data/\",\\\n",
        "                                    best_net)\n",
        "    \n",
        "    logger.info(\"Current net: %s\" % current_net)\n",
        "    logger.info(\"Previous (Best) net: %s\" % best_net)\n",
        "    \n",
        "    current_cnet = RLAgent()\n",
        "    best_cnet = RLAgent()\n",
        "    cuda = torch.cuda.is_available()\n",
        "    if cuda:\n",
        "        \n",
        "        current_cnet.cuda()\n",
        "        best_cnet.cuda()\n",
        "    \n",
        "    if not os.path.isdir(\"./evaluator_data/\"):\n",
        "        os.mkdir(\"evaluator_data\")\n",
        "    \n",
        "    if args.MCTS_num_processes > 1:\n",
        "        mp.set_start_method(\"spawn\",force=True)\n",
        "        \n",
        "        current_cnet.share_memory(); best_cnet.share_memory()\n",
        "        current_cnet.eval(); best_cnet.eval()\n",
        "        \n",
        "        checkpoint = torch.load(current_net_filename)\n",
        "        current_cnet.load_state_dict(checkpoint['state_dict'])\n",
        "        checkpoint = torch.load(best_net_filename)\n",
        "        best_cnet.load_state_dict(checkpoint['state_dict'])\n",
        "         \n",
        "        processes = []\n",
        "        if args.MCTS_num_processes > mp.cpu_count():\n",
        "            num_processes = mp.cpu_count()\n",
        "            logger.info(\"Required number of processes exceed number of CPUs! Setting MCTS_num_processes to %d\" % num_processes)\n",
        "        else:\n",
        "            num_processes = args.MCTS_num_processes\n",
        "        logger.info(\"Spawning %d processes...\" % num_processes)\n",
        "        with torch.no_grad():\n",
        "            for i in range(num_processes):\n",
        "                p = mp.Process(target=fork_process,args=(arena(current_cnet,best_cnet), args.num_evaluator_games, i))\n",
        "                p.start()\n",
        "                processes.append(p)\n",
        "            for p in processes:\n",
        "                p.join()\n",
        "               \n",
        "        wins_ratio = 0.0\n",
        "        for i in range(num_processes):\n",
        "            stats = load_pickle(\"wins_cpu_%i\" % (i))\n",
        "            wins_ratio += stats['best_win_ratio']\n",
        "        wins_ratio = wins_ratio/num_processes\n",
        "        if wins_ratio >= 0.55:\n",
        "            return iteration_2\n",
        "        else:\n",
        "            return iteration_1\n",
        "            \n",
        "    elif args.MCTS_num_processes == 1:\n",
        "        current_cnet.eval(); best_cnet.eval()\n",
        "        checkpoint = torch.load(current_net_filename)\n",
        "        current_cnet.load_state_dict(checkpoint['state_dict'])\n",
        "        checkpoint = torch.load(best_net_filename)\n",
        "        best_cnet.load_state_dict(checkpoint['state_dict'])\n",
        "        arena1 = arena(current_cnet=current_cnet, best_cnet=best_cnet)\n",
        "        arena1.evaluate(num_games=args.num_evaluator_games, cpu=0)\n",
        "        \n",
        "        stats = load_pickle(\"wins_cpu_%i\" % (0))\n",
        "        if stats.best_win_ratio >= 0.55:\n",
        "            return iteration_2\n",
        "        else:\n",
        "            return iteration_1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeXEE90Jw48v",
        "colab_type": "text"
      },
      "source": [
        "### Train agent with MCTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpsuRBiQrJiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class argval:\n",
        "    def __init__(self, args = None):\n",
        "        self.iteration = 10\n",
        "        self.total_iterations=1000\n",
        "        self.MCTS_num_processes=1\n",
        "        self.num_games_per_MCTS_process=120\n",
        "        self.temperature_MCTS=1.1\n",
        "        self.num_evaluator_games=10\n",
        "        self.neural_net_name=\"cc4_current_net_\"\n",
        "        self.batch_size=32\n",
        "        self.num_epochs=300\n",
        "        self.lr=0.001\n",
        "        self.gradient_acc_steps=1\n",
        "        self.max_norm=1.0\n",
        "args = argval()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbgGTiMQpoLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "check_dir('model_data')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCjkpkDemsa8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "41d44543-983d-43f6-b95f-13e0da6e7071"
      },
      "source": [
        "logger.info(\"Starting iteration pipeline...\")\n",
        "for i in range(args.iteration, args.total_iterations): \n",
        "    print('Running MCTS')\n",
        "    run_MCTS(args, start_idx=0, iteration=i)\n",
        "    print('Training the agent..')\n",
        "    train_connectnet(args, iteration=i, new_optim_state=True)\n",
        "    if i >= 1:\n",
        "        winner = evaluate_nets(args, i, i + 1)\n",
        "        counts = 0\n",
        "        while (winner != (i + 1)):\n",
        "            logger.info(\"Trained net didn't perform better, generating more MCTS games for retraining...\")\n",
        "            print('Running MCTS again...')\n",
        "            run_MCTS(args, start_idx=(counts + 1)*args.num_games_per_MCTS_process, iteration=i)\n",
        "            counts += 1\n",
        "            print('Training the agent...')\n",
        "            train_connectnet(args, iteration=i, new_optim_state=True)\n",
        "            winner = evaluate_nets(args, i, i + 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CPU: 0]: Game 4 POLICY:\n",
            "  [0.13130665 0.13955437 0.1419018  0.15590756 0.16401906 0.13955437\n",
            " 0.12775618]\n",
            "[Iteration: 10 CPU: 0]: Game 4 CURRENT BOARD:\n",
            " [[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' 'X' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' 'X' ' ' ' ' ' ' ' ' 'O']\n",
            " ['X' 'X' 'O' ' ' 'O' ' ' 'O']] 0\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-GKeUSXpuxi",
        "colab_type": "text"
      },
      "source": [
        "### Play A Game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxnNBlEtpxzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(net):\n",
        "    # Asks human what he/she wanna play as\n",
        "    white = None; black = None\n",
        "    while (True):\n",
        "        play_as = input(\"What do you wanna play as? (\\\"O\\\"/\\\"X\\\")? Note: \\\"O\\\" starts first, \\\"X\\\" starts second\\n\")\n",
        "        if play_as == \"O\":\n",
        "            black = net; break\n",
        "        elif play_as == \"X\":\n",
        "            white = net; break\n",
        "        else:\n",
        "            print(\"I didn't get that.\")\n",
        "    current_board = board()\n",
        "    checkmate = False\n",
        "    dataset = []\n",
        "    value = 0; t = 0.1; moves_count = 0\n",
        "    while checkmate == False and current_board.actions() != []:\n",
        "        if moves_count <= 5:\n",
        "            t = 1\n",
        "        else:\n",
        "            t = 0.1\n",
        "        moves_count += 1\n",
        "        dataset.append(copy.deepcopy(encode_board(current_board)))\n",
        "        print(current_board.current_board); print(\" \")\n",
        "        if current_board.player == 0:\n",
        "            if white != None:\n",
        "                print(\"AI is thinking........\")\n",
        "                root = UCT_search(current_board,777,white,t)\n",
        "                policy = get_policy(root, t)\n",
        "            else:\n",
        "                while(True):\n",
        "                    col = input(\"Which column do you wanna drop your piece? (Enter 1-7)\\n\")\n",
        "                    if int(col) in [1,2,3,4,5,6,7]:\n",
        "                        policy = np.zeros([7], dtype=np.float32); policy[int(col)-1] += 1\n",
        "                        break\n",
        "        elif current_board.player == 1:\n",
        "            if black != None:\n",
        "                print(\"AI is thinking.............\")\n",
        "                root = UCT_search(current_board,777,black,t)\n",
        "                policy = get_policy(root, t)\n",
        "            else:\n",
        "                while(True):\n",
        "                    col = input(\"Which column do you wanna drop your piece? (Enter 1-7)\\n\")\n",
        "                    if int(col) in [1,2,3,4,5,6,7]:\n",
        "                        policy = np.zeros([7], dtype=np.float32); policy[int(col)-1] += 1\n",
        "                        break\n",
        "        current_board = do_decode_n_move_pieces(current_board,\\\n",
        "                                                np.random.choice(np.array([0,1,2,3,4,5,6]), \\\n",
        "                                                                 p = policy)) # decode move and move piece(s)\n",
        "        if current_board.check_winner() == True: # someone wins\n",
        "            if current_board.player == 0: # black wins\n",
        "                value = -1\n",
        "            elif current_board.player == 1: # white wins\n",
        "                value = 1\n",
        "            checkmate = True\n",
        "    dataset.append(encode_board(current_board))\n",
        "    print(current_board.current_board); print(\" \")\n",
        "    if value == -1:\n",
        "        if play_as == \"O\":\n",
        "            dataset.append(f\"AI as black wins\"); print(\"YOU LOSE!!!!!!!\")\n",
        "        else:\n",
        "            dataset.append(f\"Human as black wins\"); print(\"YOU WIN!!!!!!!\")\n",
        "        return \"black\", dataset\n",
        "    elif value == 1:\n",
        "        if play_as == \"O\":\n",
        "            dataset.append(f\"Human as white wins\"); print(\"YOU WIN!!!!!!!!!!!\")\n",
        "        else:\n",
        "            dataset.append(f\"AI as white wins\"); print(\"YOU LOSE!!!!!!!\")\n",
        "        return \"white\", dataset\n",
        "    else:\n",
        "        dataset.append(\"Nobody wins\"); print(\"DRAW!!!!!\")\n",
        "        return None, dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnmXYVYDp2hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "best_net=\"cc4_current_net__iter1.pth.tar\"\n",
        "best_net_filename = os.path.join(\"./model_data/\",\\\n",
        "                                best_net)\n",
        "best_cnet = RLAgent()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    best_cnet.cuda()\n",
        "best_cnet.eval()\n",
        "checkpoint = torch.load(best_net_filename)\n",
        "best_cnet.load_state_dict(checkpoint['state_dict'])\n",
        "play_again = True\n",
        "while(play_again == True):\n",
        "    play_game(best_cnet)\n",
        "    while(True):\n",
        "        again = input(\"Do you wanna play again? (Y/N)\\n\")\n",
        "        if again.lower() in [\"y\", \"n\"]:\n",
        "            if again.lower() == \"n\":\n",
        "                play_again = False; break\n",
        "            else:\n",
        "                break\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKhFqjDFvKyX",
        "colab_type": "text"
      },
      "source": [
        "#### Thankyou, Abhishek Kumar"
      ]
    }
  ]
}